# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nJauJdkUWBdlKZSDSgCPWLK9Znr-1jAx
"""

!pip install readability-lxml
!pip install lxml_html_clean
!pip install html2text


import requests
from readability import Document
import html2text
import os

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

base_url = "https://www.fib.upc.edu/"
visited = set()
pages = {}

def extract_content(url):
    """Extrae encabezados, párrafos, listas, imágenes y enlaces de una página."""
    try:
        response = requests.get(url, timeout=5)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        content = []

        # Extraer encabezados
        for tag in ["h1", "h2", "h3", "h4", "h5", "h6"]:
            for header in soup.find_all(tag):
                content.append(f"{'#' * int(tag[1])} {header.text.strip()}")  # Markdown headers

        # Extraer párrafos
        for paragraph in soup.find_all("p"):
            content.append(paragraph.text.strip())

        # Extraer listas
        for ul in soup.find_all("ul"):
            for li in ul.find_all("li"):
                content.append(f"- {li.text.strip()}")

        # Extraer imágenes con su alt y URL
        for img in soup.find_all("img", src=True):
            img_url = urljoin(url, img["src"])
            alt_text = img.get("alt", "Imagen sin descripción")
            content.append(f"![{alt_text}]({img_url})")

        # Extraer enlaces internos
        links = set()
        for link in soup.find_all("a", href=True):
            full_url = urljoin(url, link["href"])
            if urlparse(full_url).netloc == urlparse(base_url).netloc:
                links.add(full_url)

        return "\n\n".join(content), links

    except requests.RequestException:
        return "", set()

def crawl(url, depth=3):
    """Explora el sitio web y extrae contenido de cada página interna."""
    if url in visited or depth == 0:
        return

    visited.add(url)
    text_content, links = extract_content(url)
    pages[url] = text_content

    for link in links:
        crawl(link, depth - 1)

# Iniciar rastreo
crawl(base_url)

# Guardar la información en un archivo Markdown
with open("fib_full_content.md", "w", encoding="utf-8") as f:
    for page, content in pages.items():
        f.write(f"## [{page}]({page})\n\n{content}\n\n---\n\n")

# prompt: download the fib_main.md file into my pc

from google.colab import files
files.download('fib_full_content.md')